{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296a6b98-2df7-41a9-82d1-c0fc48330d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punkt tokenizer found successfully!\n",
      "Libraries imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/salkimmich/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/salkimmich/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/salkimmich/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Advanced Pandas and NumPy with NLP Analysis and Clustering\n",
    "\n",
    "## 1. Setup and Imports\n",
    "\n",
    "import numpy as np  # NumPy for numerical operations and array manipulation\n",
    "import pandas as pd  # Pandas for data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # For visualization\n",
    "import seaborn as sns  # Enhanced visualizations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data resources with explicit verification\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Verify the punkt tokenizer is available\n",
    "try:\n",
    "    from nltk.data import find\n",
    "    find('tokenizers/punkt')\n",
    "    print(\"Punkt tokenizer found successfully!\")\n",
    "except LookupError:\n",
    "    print(\"Punkt tokenizer not found. Downloading again...\")\n",
    "    nltk.download('punkt', quiet=False)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64877de4-9e49-4e5e-9ecf-73ff0184f186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data created!\n",
      "Number of responses: 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_id</th>\n",
       "      <th>feedback_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The service at this clinic was excellent. Staf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Wait times were too long. I had to sit for ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Doctors were knowledgeable but the facility ne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   response_id                                      feedback_text\n",
       "0            1  The service at this clinic was excellent. Staf...\n",
       "1            2  Wait times were too long. I had to sit for ove...\n",
       "2            3  Doctors were knowledgeable but the facility ne..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## 2. Creating Sample Survey Data\n",
    "\n",
    "# Let's create a synthetic dataset similar to your Listen4Good NLP analysis\n",
    "np.random.seed(42)  # NumPy's random seed ensures reproducibility\n",
    "\n",
    "# Create sample survey responses\n",
    "responses = [\n",
    "    \"The service at this clinic was excellent. Staff was very responsive and caring.\",\n",
    "    \"Wait times were too long. I had to sit for over an hour before seeing a doctor.\",\n",
    "    \"Doctors were knowledgeable but the facility needs updating.\",\n",
    "    \"I appreciate how the staff explained everything clearly to me.\",\n",
    "    \"The parking situation is terrible. Had to walk far with my injured leg.\",\n",
    "    \"Very clean facility and professional staff. Would recommend to others.\",\n",
    "    \"Communication could be improved. I wasn't notified about my appointment change.\",\n",
    "    \"The children's play area was a nice touch while waiting for our appointment.\",\n",
    "    \"Billing department made multiple errors and was difficult to reach.\",\n",
    "    \"The new patient portal is confusing and hard to navigate.\",\n",
    "    \"Nurses were attentive and made me feel comfortable during my procedure.\",\n",
    "    \"I was surprised by the unexpected costs that weren't explained beforehand.\",\n",
    "    \"The follow-up care instructions were clear and helpful.\",\n",
    "    \"Front desk staff needs more training on insurance procedures.\",\n",
    "    \"I felt rushed during my appointment. Doctor barely spent 5 minutes with me.\",\n",
    "    \"The telehealth option saved me so much time. Very convenient.\",\n",
    "    \"Equipment seemed outdated compared to other facilities I've visited.\",\n",
    "    \"Staff was friendly but seemed understaffed and overworked.\",\n",
    "    \"The specialty care I received for my condition was exceptional.\",\n",
    "    \"Difficult to get appointments in a reasonable timeframe.\",\n",
    "    \"The clinic location is convenient but hours are too limited.\",\n",
    "    \"My treatment plan was thoroughly explained and I felt involved in decisions.\",\n",
    "    \"Waiting room was overcrowded with sick patients too close together.\",\n",
    "    \"Reception staff was rude when I asked questions about my insurance.\",\n",
    "    \"The pharmacy coordination with my doctor worked seamlessly.\",\n",
    "    \"More diverse language options would help many patients in this community.\",\n",
    "    \"Billing was straightforward and transparent. No surprises.\",\n",
    "    \"The pediatric nurse was amazing with my anxious child.\",\n",
    "    \"Too many forms to fill out that asked for redundant information.\",\n",
    "    \"The online scheduling system repeatedly crashed when I tried to use it.\"\n",
    "]\n",
    "\n",
    "# Create demographic and metadata\n",
    "# IMPORTANT: Using NumPy's random functions for generating synthetic data\n",
    "# is much faster than Python's random module for large datasets\n",
    "demographics = []\n",
    "for i in range(len(responses)):\n",
    "    # np.random.choice is faster than random.choice for selecting from arrays\n",
    "    age_group = np.random.choice(['18-24', '25-34', '35-44', '45-54', '55-64', '65+'])\n",
    "    gender = np.random.choice(['Female', 'Male', 'Other', 'Prefer not to say'])\n",
    "    visit_type = np.random.choice(['Primary Care', 'Specialist', 'Emergency', 'Routine Checkup', 'Procedure'])\n",
    "    satisfaction = np.random.randint(1, 11)  # 1-10 satisfaction score\n",
    "    \n",
    "    demographics.append({\n",
    "        'response_id': i+1,\n",
    "        'age_group': age_group,\n",
    "        'gender': gender,\n",
    "        'visit_type': visit_type,\n",
    "        'satisfaction_score': satisfaction\n",
    "    })\n",
    "\n",
    "# Create DataFrames - Pandas makes it easy to organize structured data\n",
    "# WHY PANDAS: We use DataFrames here because they handle mixed data types\n",
    "# (text, categories, numbers) in a tabular structure with labeled columns\n",
    "df_responses = pd.DataFrame({\n",
    "    'response_id': range(1, len(responses)+1),\n",
    "    'feedback_text': responses\n",
    "})\n",
    "\n",
    "df_demographics = pd.DataFrame(demographics)\n",
    "\n",
    "print(\"Sample data created!\")\n",
    "print(f\"Number of responses: {len(df_responses)}\")\n",
    "df_responses.head(3)  # Pandas .head() method is great for quickly viewing data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13a5f109-3f1b-40c4-8b94-f3a2663f856f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Satisfaction Scores (first 10): [8 3 4 2 5 6 1 3 9 7]\n",
      "Shape: (30,)\n",
      "Data Type: int64\n",
      "\n",
      "Basic Statistics with NumPy:\n",
      "Mean satisfaction: 5.166666666666667\n",
      "Median satisfaction: 4.0\n",
      "Standard deviation: 3.066847820736392\n",
      "Min score: 1\n",
      "Max score: 10\n",
      "\n",
      "Frequency Distribution:\n",
      "Score 1: 3 responses\n",
      "Score 2: 5 responses\n",
      "Score 3: 4 responses\n",
      "Score 4: 4 responses\n",
      "Score 5: 1 responses\n",
      "Score 6: 1 responses\n",
      "Score 7: 3 responses\n",
      "Score 8: 2 responses\n",
      "Score 9: 4 responses\n",
      "Score 10: 3 responses\n",
      "\n",
      "Percentiles:\n",
      "25th percentile: 2.25\n",
      "50th percentile (median): 4.0\n",
      "75th percentile: 8.0\n",
      "90th percentile: 9.100000000000001\n",
      "\n",
      "Number of high scores (8-10): 9\n",
      "Number of low scores (1-3): 12\n",
      "Percentage of high scores: 30.0%\n"
     ]
    }
   ],
   "source": [
    "## 3. NumPy Fundamentals with Data Preparation\n",
    "\n",
    "# WHY NUMPY: Convert Pandas Series to NumPy array for faster numerical operations\n",
    "# NumPy arrays are more memory-efficient and perform calculations faster\n",
    "satisfaction_scores = np.array(df_demographics['satisfaction_score'])\n",
    "\n",
    "print(\"\\nSatisfaction Scores (first 10):\", satisfaction_scores[:10])\n",
    "print(\"Shape:\", satisfaction_scores.shape)  # NumPy arrays have convenient shape properties\n",
    "print(\"Data Type:\", satisfaction_scores.dtype)  # NumPy manages data types efficiently\n",
    "\n",
    "# Basic NumPy statistics on our data\n",
    "# WHY NUMPY: These statistical functions are vectorized (no loops needed)\n",
    "# and much faster than calculating these manually\n",
    "print(\"\\nBasic Statistics with NumPy:\")\n",
    "print(\"Mean satisfaction:\", np.mean(satisfaction_scores))\n",
    "print(\"Median satisfaction:\", np.median(satisfaction_scores))\n",
    "print(\"Standard deviation:\", np.std(satisfaction_scores))\n",
    "print(\"Min score:\", np.min(satisfaction_scores))\n",
    "print(\"Max score:\", np.max(satisfaction_scores))\n",
    "\n",
    "# Create a frequency distribution using NumPy\n",
    "# IMPORTANT: np.unique efficiently finds unique values and counts them\n",
    "# This is much faster than using Python loops or collections.Counter\n",
    "unique_scores, counts = np.unique(satisfaction_scores, return_counts=True)\n",
    "print(\"\\nFrequency Distribution:\")\n",
    "for score, count in zip(unique_scores, counts):\n",
    "    print(f\"Score {score}: {count} responses\")\n",
    "\n",
    "# Converting to percentiles with NumPy\n",
    "# WHY NUMPY: np.percentile calculates multiple percentiles in one pass through the data\n",
    "percentiles = np.percentile(satisfaction_scores, [25, 50, 75, 90])\n",
    "print(\"\\nPercentiles:\")\n",
    "print(f\"25th percentile: {percentiles[0]}\")\n",
    "print(f\"50th percentile (median): {percentiles[1]}\")\n",
    "print(f\"75th percentile: {percentiles[2]}\")\n",
    "print(f\"90th percentile: {percentiles[3]}\")\n",
    "\n",
    "# Using NumPy for filtering and conditional operations\n",
    "# WHY NUMPY: Boolean indexing in NumPy is very efficient and readable\n",
    "# Compare to having to write a loop or comprehension in pure Python\n",
    "high_scores = satisfaction_scores[satisfaction_scores >= 8]\n",
    "low_scores = satisfaction_scores[satisfaction_scores <= 3]\n",
    "\n",
    "print(f\"\\nNumber of high scores (8-10): {len(high_scores)}\")\n",
    "print(f\"Number of low scores (1-3): {len(low_scores)}\")\n",
    "print(f\"Percentage of high scores: {len(high_scores)/len(satisfaction_scores)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621d8c1d-ee2d-4bac-b712-5fdc83b8d49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged DataFrame (first 3 rows):\n",
      "   response_id                                      feedback_text age_group  \\\n",
      "0            1  The service at this clinic was excellent. Staf...     45-54   \n",
      "1            2  Wait times were too long. I had to sit for ove...     55-64   \n",
      "2            3  Doctors were knowledgeable but the facility ne...     35-44   \n",
      "\n",
      "   gender  visit_type  satisfaction_score  \n",
      "0  Female   Emergency                   8  \n",
      "1  Female  Specialist                   3  \n",
      "2   Other   Procedure                   4  \n",
      "\n",
      "Dataset Overview:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30 entries, 0 to 29\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   response_id         30 non-null     int64 \n",
      " 1   feedback_text       30 non-null     object\n",
      " 2   age_group           30 non-null     object\n",
      " 3   gender              30 non-null     object\n",
      " 4   visit_type          30 non-null     object\n",
      " 5   satisfaction_score  30 non-null     int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 1.5+ KB\n",
      "None\n",
      "\n",
      "Summary Statistics:\n",
      "        response_id                                      feedback_text  \\\n",
      "count     30.000000                                                 30   \n",
      "unique          NaN                                                 30   \n",
      "top             NaN  The service at this clinic was excellent. Staf...   \n",
      "freq            NaN                                                  1   \n",
      "mean      15.500000                                                NaN   \n",
      "std        8.803408                                                NaN   \n",
      "min        1.000000                                                NaN   \n",
      "25%        8.250000                                                NaN   \n",
      "50%       15.500000                                                NaN   \n",
      "75%       22.750000                                                NaN   \n",
      "max       30.000000                                                NaN   \n",
      "\n",
      "       age_group             gender  visit_type  satisfaction_score  \n",
      "count         30                 30          30           30.000000  \n",
      "unique         6                  4           5                 NaN  \n",
      "top        45-54  Prefer not to say  Specialist                 NaN  \n",
      "freq           8                 10          10                 NaN  \n",
      "mean         NaN                NaN         NaN            5.166667  \n",
      "std          NaN                NaN         NaN            3.119276  \n",
      "min          NaN                NaN         NaN            1.000000  \n",
      "25%          NaN                NaN         NaN            2.250000  \n",
      "50%          NaN                NaN         NaN            4.000000  \n",
      "75%          NaN                NaN         NaN            8.000000  \n",
      "max          NaN                NaN         NaN           10.000000  \n",
      "\n",
      "Satisfaction by Visit Type:\n",
      "                     mean  median  count       std\n",
      "visit_type                                        \n",
      "Routine Checkup  6.428571     7.0      7  3.154739\n",
      "Specialist       5.800000     4.5     10  2.820559\n",
      "Primary Care     4.500000     4.5      2  4.949747\n",
      "Emergency        4.400000     3.0      5  2.966479\n",
      "Procedure        3.500000     2.0      6  3.331666\n",
      "\n",
      "Satisfaction by Age Group:\n",
      "            mean  median  count       std\n",
      "age_group                                \n",
      "25-34      7.000     7.0      1       NaN\n",
      "65+        6.250     6.5      4  3.862210\n",
      "45-54      5.625     6.0      8  3.113909\n",
      "18-24      5.500     4.5      8  3.464102\n",
      "35-44      4.000     3.5      4  2.160247\n",
      "55-64      3.600     3.0      5  3.286335\n",
      "\n",
      "Satisfaction by Gender:\n",
      "                       mean  median  count       std\n",
      "gender                                              \n",
      "Female             5.714286     7.0      7  3.147183\n",
      "Male               3.166667     3.0      6  1.329160\n",
      "Other              6.285714     8.0      7  3.545621\n",
      "Prefer not to say  5.200000     4.5     10  3.392803\n",
      "\n",
      "Cross-tabulation of Visit Type and Satisfaction Category:\n",
      "satisfaction_category  High (8-10)  Low (1-4)  Medium (5-7)\n",
      "visit_type                                                 \n",
      "Emergency                     20.0       60.0          20.0\n",
      "Primary Care                  50.0       50.0           0.0\n",
      "Procedure                     16.7       83.3           0.0\n",
      "Routine Checkup               42.9       28.6          28.6\n",
      "Specialist                    30.0       50.0          20.0\n"
     ]
    }
   ],
   "source": [
    "## 4. Pandas Data Manipulation and Analysis\n",
    "\n",
    "# Merging our response and demographic DataFrames\n",
    "# WHY PANDAS: pd.merge provides SQL-like join operations that would be complex to implement manually\n",
    "df_merged = pd.merge(df_responses, df_demographics, on='response_id')\n",
    "print(\"\\nMerged DataFrame (first 3 rows):\")\n",
    "print(df_merged.head(3))\n",
    "\n",
    "# Basic exploratory data analysis with Pandas\n",
    "# IMPORTANT: Pandas provides simple methods for data exploration\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(df_merged.info())  # Shows data types and missing values count\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "# WHY PANDAS: describe() calculates multiple statistics at once,\n",
    "# handling different data types appropriately\n",
    "print(df_merged.describe(include='all'))\n",
    "\n",
    "# Group analysis with Pandas\n",
    "# WHY PANDAS: groupby() is a powerful operation that splits data into groups\n",
    "# and performs aggregate operations in one step\n",
    "print(\"\\nSatisfaction by Visit Type:\")\n",
    "visit_satisfaction = df_merged.groupby('visit_type')['satisfaction_score'].agg(['mean', 'median', 'count', 'std'])\n",
    "print(visit_satisfaction.sort_values('mean', ascending=False))  # Sorting with Pandas is straightforward\n",
    "\n",
    "print(\"\\nSatisfaction by Age Group:\")\n",
    "age_satisfaction = df_merged.groupby('age_group')['satisfaction_score'].agg(['mean', 'median', 'count', 'std'])\n",
    "print(age_satisfaction.sort_values('mean', ascending=False))\n",
    "\n",
    "print(\"\\nSatisfaction by Gender:\")\n",
    "gender_satisfaction = df_merged.groupby('gender')['satisfaction_score'].agg(['mean', 'median', 'count', 'std'])\n",
    "print(gender_satisfaction)\n",
    "\n",
    "# Create satisfaction categories for analysis\n",
    "# WHY PANDAS: apply() lets us run functions on every row or column efficiently\n",
    "def categorize_satisfaction(score):\n",
    "    if score >= 8:\n",
    "        return 'High (8-10)'\n",
    "    elif score >= 5:\n",
    "        return 'Medium (5-7)'\n",
    "    else:\n",
    "        return 'Low (1-4)'\n",
    "\n",
    "df_merged['satisfaction_category'] = df_merged['satisfaction_score'].apply(categorize_satisfaction)\n",
    "\n",
    "# Cross-tabulation with Pandas\n",
    "# WHY PANDAS: crosstab provides an efficient way to count frequencies across two variables\n",
    "print(\"\\nCross-tabulation of Visit Type and Satisfaction Category:\")\n",
    "visit_sat_crosstab = pd.crosstab(\n",
    "    df_merged['visit_type'], \n",
    "    df_merged['satisfaction_category'],\n",
    "    normalize='index'  # Normalize to get percentages by row\n",
    ") * 100  # Convert to percentages\n",
    "\n",
    "print(visit_sat_crosstab.round(1))  # Pandas makes rounding easy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c21f1387-7676-408d-b0cf-5d1642e2fde7",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/salkimmich/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to our feedback text\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# WHY PANDAS: apply() method allows applying a function to each value in a Series\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# This avoids writing explicit loops and makes the code more readable\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m df_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeedback_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal vs Processed Text (first 3 examples):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4919\u001b[0m         func,\n\u001b[1;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Tokenize\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[1;32m     18\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/salkimmich/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "## 5. Text Preprocessing for NLP Analysis\n",
    "\n",
    "# Define text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to our feedback text\n",
    "# WHY PANDAS: apply() method allows applying a function to each value in a Series\n",
    "# This avoids writing explicit loops and makes the code more readable\n",
    "df_merged['processed_text'] = df_merged['feedback_text'].apply(preprocess_text)\n",
    "\n",
    "print(\"\\nOriginal vs Processed Text (first 3 examples):\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df_merged['feedback_text'].iloc[i]}\")\n",
    "    print(f\"Processed: {df_merged['processed_text'].iloc[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176bb63-57c6-43df-b1ad-2a377afeb2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Feature Extraction with TF-IDF\n",
    "\n",
    "# Create TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=100,  # Limit to top 100 features for this example\n",
    "    min_df=2           # Ignore terms that appear in less than 2 documents\n",
    ")\n",
    "\n",
    "# Fit and transform the processed text\n",
    "# IMPORTANT: The output is a sparse matrix (not a standard NumPy array)\n",
    "# This is more memory efficient for text data with many zeros\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_merged['processed_text'])\n",
    "\n",
    "# Convert to a pandas DataFrame for easier inspection\n",
    "# WHY PANDAS: Converting to DataFrame adds column names and makes the data more interpretable\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),  # Convert sparse matrix to dense NumPy array\n",
    "    columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(\"\\nTF-IDF Matrix Shape:\", tfidf_matrix.shape)\n",
    "print(\"Sample of TF-IDF Features (first 5 columns, first 3 rows):\")\n",
    "print(tfidf_df.iloc[:3, :5])  # Pandas indexing makes it easy to view subsets of data\n",
    "\n",
    "# Get the most important terms by their TF-IDF scores\n",
    "# WHY NUMPY: np.sum along axis=0 efficiently sums columns\n",
    "feature_importance = np.sum(tfidf_matrix.toarray(), axis=0)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame of terms and their importance\n",
    "# WHY PANDAS: DataFrames make it easy to organize and sort results\n",
    "term_importance_df = pd.DataFrame({\n",
    "    'term': feature_names,\n",
    "    'importance': feature_importance\n",
    "})\n",
    "\n",
    "print(\"\\nTop 10 Most Important Terms:\")\n",
    "print(term_importance_df.sort_values('importance', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35d244-2b2e-423b-9355-7487a2a20bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 7. Clustering Analysis with K-means\n",
    "\n",
    "# Determine optimal number of clusters using silhouette scores\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)  # Try 2 to 10 clusters\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "    \n",
    "    # WHY NUMPY: Silhouette score calculation uses efficient array operations\n",
    "    silhouette_avg = silhouette_score(tfidf_matrix, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {k}, the silhouette score is {silhouette_avg:.3f}\")\n",
    "\n",
    "# Find the optimal k\n",
    "# WHY NUMPY: np.argmax finds the index of the maximum value in an array efficiently\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nOptimal number of clusters based on silhouette score: {optimal_k}\")\n",
    "\n",
    "# Apply KMeans with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "# WHY PANDAS: We can easily add the cluster assignments as a new column in our DataFrame\n",
    "df_merged['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# Analyze clusters\n",
    "# WHY PANDAS: value_counts() efficiently counts occurrences of each unique value\n",
    "cluster_counts = df_merged['cluster'].value_counts().sort_index()\n",
    "print(\"\\nNumber of responses in each cluster:\")\n",
    "print(cluster_counts)\n",
    "\n",
    "# Calculate mean satisfaction by cluster\n",
    "# WHY PANDAS: groupby() lets us easily calculate statistics for each cluster\n",
    "cluster_satisfaction = df_merged.groupby('cluster')['satisfaction_score'].mean().sort_index()\n",
    "print(\"\\nMean satisfaction score by cluster:\")\n",
    "print(cluster_satisfaction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd28d16-3c73-45e1-b927-95a40d368321",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Visualizing the Clusters with PCA\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "# WHY NUMPY: PCA works directly with NumPy arrays and uses efficient linear algebra operations\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "# WHY PANDAS: Organizing our visualization data in a DataFrame makes plotting easier\n",
    "plot_df = pd.DataFrame({\n",
    "    'x': pca_result[:, 0],  # First principal component\n",
    "    'y': pca_result[:, 1],  # Second principal component\n",
    "    'cluster': df_merged['cluster'],\n",
    "    'satisfaction': df_merged['satisfaction_score']\n",
    "})\n",
    "\n",
    "print(\"\\nPCA Results (first 5 rows):\")\n",
    "print(plot_df.head())\n",
    "\n",
    "# Plotting code (uncomment to use in Jupyter)\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.scatterplot(data=plot_df, x='x', y='y', hue='cluster', palette='viridis', \n",
    "#                size='satisfaction', sizes=(20, 200), alpha=0.7)\n",
    "# plt.title('PCA Visualization of Response Clusters')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d8ac1f-652e-49d8-8ac4-5ec49cf314bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Understanding Cluster Content\n",
    "\n",
    "# Function to get top terms for each cluster\n",
    "def get_cluster_top_terms(kmeans_model, vectorizer, cluster_id, top_n=10):\n",
    "    # Get cluster center\n",
    "    # WHY NUMPY: Cluster centers are stored as NumPy arrays for efficient computation\n",
    "    center = kmeans_model.cluster_centers_[cluster_id]\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get indices of top n values in the cluster center\n",
    "    # WHY NUMPY: argsort() efficiently returns indices that would sort the array\n",
    "    # Using negative indices and reversing gets us the highest values\n",
    "    top_indices = center.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    # Get the terms and their weights\n",
    "    top_terms = [(feature_names[i], center[i]) for i in top_indices]\n",
    "    \n",
    "    return top_terms\n",
    "\n",
    "# Get top terms for each cluster\n",
    "print(\"\\nTop Terms by Cluster:\")\n",
    "for i in range(optimal_k):\n",
    "    top_terms = get_cluster_top_terms(kmeans, tfidf_vectorizer, i)\n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    for term, weight in top_terms:\n",
    "        print(f\"  {term}: {weight:.3f}\")\n",
    "\n",
    "# Extract sample responses from each cluster\n",
    "# WHY PANDAS: Boolean indexing makes it easy to filter rows by cluster\n",
    "print(\"\\nSample Responses from Each Cluster:\")\n",
    "for i in range(optimal_k):\n",
    "    cluster_resp = df_merged[df_merged['cluster'] == i]\n",
    "    print(f\"\\nCluster {i} (n={len(cluster_resp)}, avg satisfaction={cluster_resp['satisfaction_score'].mean():.2f}):\")\n",
    "    \n",
    "    # Show 2 example responses from this cluster\n",
    "    # WHY PANDAS: sample() method easily selects random rows\n",
    "    examples = cluster_resp.sample(min(2, len(cluster_resp)))\n",
    "    for j, row in examples.iterrows():\n",
    "        print(f\"  - \\\"{row['feedback_text']}\\\" (satisfaction: {row['satisfaction_score']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0803991-30db-4161-a0cd-749898eb0cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Putting It All Together: Insights Dashboard\n",
    "\n",
    "# Create a summary of our findings for a dashboard\n",
    "# WHY PANDAS: Methods like mean(), value_counts(), and to_dict() make summarizing data easy\n",
    "summary = {\n",
    "    'total_responses': len(df_merged),\n",
    "    'avg_satisfaction': df_merged['satisfaction_score'].mean(),\n",
    "    'satisfaction_distribution': df_merged['satisfaction_category'].value_counts().to_dict(),\n",
    "    'visit_types': df_merged['visit_type'].value_counts().to_dict(),\n",
    "    'num_clusters': optimal_k,\n",
    "    'cluster_sizes': cluster_counts.to_dict(),\n",
    "    'cluster_satisfaction': cluster_satisfaction.to_dict()\n",
    "}\n",
    "\n",
    "print(\"\\n=== INSIGHTS DASHBOARD ===\")\n",
    "print(f\"Total Responses Analyzed: {summary['total_responses']}\")\n",
    "print(f\"Average Satisfaction Score: {summary['avg_satisfaction']:.2f}/10\")\n",
    "\n",
    "print(\"\\nSatisfaction Distribution:\")\n",
    "for category, count in summary['satisfaction_distribution'].items():\n",
    "    percentage = count / summary['total_responses'] * 100\n",
    "    print(f\"  {category}: {count} responses ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nVisit Type Distribution:\")\n",
    "for visit_type, count in summary['visit_types'].items():\n",
    "    percentage = count / summary['total_responses'] * 100\n",
    "    print(f\"  {visit_type}: {count} responses ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nIdentified {summary['num_clusters']} distinct feedback themes:\")\n",
    "for cluster, count in summary['cluster_sizes'].items():\n",
    "    sat = summary['cluster_satisfaction'][cluster]\n",
    "    percentage = count / summary['total_responses'] * 100\n",
    "    print(f\"  Cluster {cluster}: {count} responses ({percentage:.1f}%), Avg Satisfaction: {sat:.2f}/10\")\n",
    "\n",
    "print(\"\\nRecommended Actions:\")\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_sat = summary['cluster_satisfaction'][cluster]\n",
    "    if cluster_sat < 5:\n",
    "        print(f\"  - Address issues in Cluster {cluster} (low satisfaction area)\")\n",
    "    elif cluster_sat > 8:\n",
    "        print(f\"  - Leverage strengths from Cluster {cluster} (high satisfaction area)\")\n",
    "\n",
    "print(\"\\nAnalysis Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
