{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a36c14",
   "metadata": {},
   "source": [
    "# Python 103 â€“ Applied AI Readiness for Healthcare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2cabd6",
   "metadata": {},
   "source": [
    "## Course Overview\n",
    "This notebook builds on your skills from Python 101 and 102 to introduce essential tools for building basic AI pipelines. You will learn how to preprocess clinical data, work with machine learning models using `scikit-learn`, and retrieve data using APIs. The notebook concludes with a mini-project to structure a small data pipeline or model prototype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1022e8a8",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Understand the basics of machine learning with `scikit-learn`\n",
    "- Preprocess patient data (normalization, encoding, scaling)\n",
    "- Build a simple machine learning pipeline\n",
    "- Retrieve data using public APIs\n",
    "- Organize code and results in Jupyter Notebooks\n",
    "- Complete a prototype mini-project using real-world clinical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1205051e",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Access\n",
    "We will continue using the `patjs/patient1` dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9fe807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('patjs/patient1')\n",
    "patients_df = data['patients'].to_pandas()\n",
    "encounters_df = data['encounters'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b049b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(patients_df.head())\n",
    "print(encounters_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd9849",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Data for Machine Learning\n",
    "We will use age and gender to predict whether a patient has had more than one encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efecae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: calculate age\n",
    "patients_df['BIRTHDATE'] = pd.to_datetime(patients_df['BIRTHDATE'])\n",
    "patients_df['AGE'] = 2024 - patients_df['BIRTHDATE'].dt.year\n",
    "# Count encounters\n",
    "encounter_counts = encounters_df.groupby('PATIENT').size().reset_index(name='NUM_ENCOUNTERS')\n",
    "# Merge data\n",
    "merged_df = pd.merge(patients_df, encounter_counts, left_on='Id', right_on='PATIENT', how='inner')\n",
    "# Create binary target\n",
    "merged_df['MULTI_VISIT'] = (merged_df['NUM_ENCOUNTERS'] > 1).astype(int)\n",
    "print(merged_df[['AGE', 'GENDER', 'MULTI_VISIT']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a450afd5",
   "metadata": {},
   "source": [
    "### Encode categorical variables and scale numerical inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "merged_df['GENDER_ENC'] = le.fit_transform(merged_df['GENDER'])\n",
    "X = merged_df[['AGE', 'GENDER_ENC']]\n",
    "y = merged_df['MULTI_VISIT']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c740579",
   "metadata": {},
   "source": [
    "## 3. Building a scikit-learn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Model Accuracy:\", pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c3cfa",
   "metadata": {},
   "source": [
    "## 4. Calling APIs for External Data\n",
    "You can use APIs to augment datasets or interact with cloud services.\n",
    "Here's how you might query a public health API (example placeholder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6407da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get('https://api.github.com/repos/huggingface/datasets')\n",
    "if response.status_code == 200:\n",
    "    print(response.json()['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bdacc1",
   "metadata": {},
   "source": [
    "## 5. Organizing Work in Jupyter Notebooks\n",
    "- Use markdown for documenting process\n",
    "- Use meaningful headings and comments\n",
    "- Store intermediate outputs and versions\n",
    "- Export notebooks using File > Download As"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f060f40a",
   "metadata": {},
   "source": [
    "## 6. Mini-Project: Build a Simple Risk Model\n",
    "Using the same `merged_df`:\n",
    "- Create a new feature based on `AGE` and `NUM_ENCOUNTERS`\n",
    "- Train a decision tree classifier using `sklearn`\n",
    "- Evaluate its performance and document your process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c63d85",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- You learned how to structure ML pipelines\n",
    "- Applied encoding, scaling, and basic modeling\n",
    "- Used APIs to retrieve external data\n",
    "- Practiced organizing an AI development notebook\n",
    "\n",
    "You're now ready to start working on real-world AI applications using Python."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
